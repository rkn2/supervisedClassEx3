{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we’ve only talked about uncertainty estimates in binary classification. But the decision_function and predict_proba methods also work in the multiclass setting. Let’s apply them on the Iris dataset, which is a three-class classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.01, random_state=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, random_state=42)\n",
    "\n",
    "gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision function shape: (38, 3)\n",
      "Decision function:\n",
      "[[-1.995715    0.04758267 -1.92720695]\n",
      " [ 0.06146394 -1.90755736 -1.92793758]\n",
      " [-1.99058203 -1.87637861  0.09686725]\n",
      " [-1.995715    0.04758267 -1.92720695]\n",
      " [-1.99730159 -0.13469108 -1.20341483]\n",
      " [ 0.06146394 -1.90755736 -1.92793758]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision function shape:\", gbrt.decision_function(X_test).shape)\n",
    "# plot the first few entries of the decision function\n",
    "print(\"Decision function:\")\n",
    "print(gbrt.decision_function(X_test)[:6, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multiclass case, the decision_function has the shape (n_samples, n_classes) and each column provides a “certainty score” for each class, where a large score means that a class is more likely and a small score means the class is less likely. You can recover the predictions from these scores by finding the maximum entry for each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argmax of decision function:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n",
      "Predictions:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Argmax of decision function:\")\n",
    "print(np.argmax(gbrt.decision_function(X_test), axis=1))\n",
    "print(\"Predictions:\")\n",
    "print(gbrt.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of predict_proba has the same shape, (n_samples, n_classes). Again, the probabilities for the possible classes for each data point sum to 1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities:\n",
      "[[0.10217718 0.78840034 0.10942248]\n",
      " [0.78347147 0.10936745 0.10716108]\n",
      " [0.09818072 0.11005864 0.79176065]\n",
      " [0.10217718 0.78840034 0.10942248]\n",
      " [0.10360005 0.66723901 0.22916094]\n",
      " [0.78347147 0.10936745 0.10716108]]\n",
      "Sums: [1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# show the first few entries of predict_proba\n",
    "print(\"Predicted probabilities:\")\n",
    "print(gbrt.predict_proba(X_test)[:6])\n",
    "# show that sums across rows are one\n",
    "print(\"Sums:\", gbrt.predict_proba(X_test)[:6].sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again recover the predictions by computing the argmax of predict_proba:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argmax of predicted probabilities:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n",
      "Predictions:\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Argmax of predicted probabilities:\")\n",
    "print(np.argmax(gbrt.predict_proba(X_test), axis=1))\n",
    "print(\"Predictions:\")\n",
    "print(gbrt.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, predict_proba and decision_function always have shape (n_samples, n_classes)—apart from decision_function in the special binary case. In the binary case, decision_function only has one column, corresponding to the “positive” class classes_[1]. This is mostly for historical reasons.\n",
    "\n",
    "You can recover the prediction when there are n_classes many columns by computing the argmax across columns. Be careful, though, if your classes are strings, or you use integers but they are not consecutive and starting from 0. If you want to compare results obtained with predict to results obtained via decision_function or predict_proba, make sure to use the classes_ attribute of the classifier to get the actual class names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique classes in training data: ['setosa' 'versicolor' 'virginica']\n",
      "predictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa'\n",
      " 'versicolor' 'virginica' 'versicolor' 'versicolor']\n",
      "argmax of decision function: [1 0 2 1 1 0 1 2 1 1]\n",
      "argmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor' 'setosa'\n",
      " 'versicolor' 'virginica' 'versicolor' 'versicolor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beccanapolitano/.conda/envs/supervisedClassEx3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# represent each target by its class name in the iris dataset\n",
    "named_target = iris.target_names[y_train]\n",
    "logreg.fit(X_train, named_target)\n",
    "print(\"unique classes in training data:\", logreg.classes_)\n",
    "print(\"predictions:\", logreg.predict(X_test)[:10])\n",
    "argmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=1)\n",
    "print(\"argmax of decision function:\", argmax_dec_func[:10])\n",
    "print(\"argmax combined with classes_:\",\n",
    "      logreg.classes_[argmax_dec_func][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
