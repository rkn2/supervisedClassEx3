{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started this section on supervised learning with a discussion of model complexity, then discussed generalization, or \n",
    "learning a model that is able to perform well on new, previously unseen data. This led us to the concepts of underfitting, which describes a model that cannot capture the variations present in the training data, and overfitting, which describes a model that focuses too much on the training data and is not able to generalize to new data very well.\n",
    "\n",
    "We then discussed a wide array of machine learning models for classification and regression, what their advantages and disadvantages are, and how to control model complexity for each of them. We saw that for many of the algorithms, setting the right parameters is important for good performance. Some of the algorithms are also sensitive to how we represent the input data, and in particular to how the features are scaled. Therefore, blindly applying an algorithm to a dataset without understanding the assumptions the model makes and the meanings of the parameter settings will rarely lead to an accurate model.\n",
    "\n",
    "These past three classes contain a lot of information about the algorithms, and it is not necessary for you to remember\n",
    " all of these details for the following classes. However, some knowledge of the models described here—and which to use \n",
    " in a specific situation—is important for successfully applying machine learning in practice. Here is a quick summary of when to use each model:\n",
    "\n",
    "*Nearest neighbors\n",
    "For small datasets, good as a baseline, easy to explain.\n",
    "\n",
    "*Linear models\n",
    "Go-to as a first algorithm to try, good for very large datasets, good for very high-dimensional data.\n",
    "\n",
    "*Naive Bayes\n",
    "Only for classification. Even faster than linear models, good for very large datasets and high-dimensional data. Often less accurate than linear models.\n",
    "\n",
    "*Decision trees\n",
    "Very fast, don’t need scaling of the data, can be visualized and easily explained.\n",
    "\n",
    "*Random forests\n",
    "Nearly always perform better than a single decision tree, very robust and powerful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\n",
    "\n",
    "*Gradient boosted decision trees\n",
    "Often slightly more accurate than random forests. Slower to train but faster to predict than random forests, and smaller in memory. Need more parameter tuning than random forests.\n",
    "\n",
    "*Support vector machines\n",
    "Powerful for medium-sized datasets of features with similar meaning. Require scaling of data, sensitive to parameters.\n",
    "\n",
    "*Neural networks\n",
    "Can build very complex models, particularly for large datasets. Sensitive to scaling of the data and to the choice of parameters. Large models need a long time to train.\n",
    "\n",
    "When working with a new dataset, it is in general a good idea to start with a simple model, such as a linear model or a naive Bayes or nearest neighbors classifier, and see how far you can get. After understanding more about the data, you can consider moving to an algorithm that can build more complex models, such as random forests, gradient boosted decision trees, SVMs, or neural networks.\n",
    "\n",
    "You should now be in a position where you have some idea of how to apply, tune, and analyze the models we discussed \n",
    "here. In the past three classes, we focused on the binary classification case, as this is usually easiest to \n",
    "understand. Most of the \n",
    "algorithms presented have classification and regression variants, however, and all of the classification algorithms support both binary and multiclass classification. Try applying any of these algorithms to the built-in datasets in scikit-learn, like the boston_housing or diabetes datasets for regression, or the digits dataset for multiclass classification. Playing around with the algorithms on different datasets will give you a better feel for how long they need to train, how easy it is to analyze the models, and how sensitive they are to the representation of the data.\n",
    "\n",
    "While we analyzed the consequences of different parameter settings for the algorithms we investigated, building a \n",
    "model that actually generalizes well to new data in production is a bit trickier than that. We will see how to \n",
    "properly adjust parameters and how to find good parameters automatically in a future class.\n",
    "\n",
    "First, though, we will dive in more detail into unsupervised learning and preprocessing.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}